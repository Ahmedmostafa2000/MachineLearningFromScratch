{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import e,log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the mathematical formulas for the logistic regression implementation you provided:\n",
    "\n",
    "1. **Sigmoid Function**:\n",
    "   \\[ \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} \\]\n",
    "   - This function squashes the input \\( z \\) to a range between 0 and 1, which is interpreted as the probability of the positive class.\n",
    "\n",
    "2. **Binary Cross-Entropy (BCE) Cost Function**:\n",
    "   \\[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(a^{(i)}) + (1 - y^{(i)}) \\log(1 - a^{(i)})] \\]\n",
    "   - \\( m \\) is the number of training examples.\n",
    "   - \\( y^{(i)} \\) is the actual label of the \\( i \\)-th training example.\n",
    "   - \\( a^{(i)} \\) is the predicted probability that the \\( i \\)-th example belongs to class 1.\n",
    "\n",
    "3. **Gradient of BCE Cost Function with Respect to Weights**:\n",
    "   \\[ \\frac{\\partial J(\\theta)}{\\partial \\mathbf{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (a^{(i)} - y^{(i)}) \\mathbf{x}^{(i)} \\]\n",
    "   - \\( \\mathbf{w} \\) are the weights.\n",
    "   - \\( \\mathbf{x}^{(i)} \\) is the feature vector of the \\( i \\)-th training example.\n",
    "\n",
    "4. **Gradient of BCE Cost Function with Respect to Bias**:\n",
    "   \\[ \\frac{\\partial J(\\theta)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (a^{(i)} - y^{(i)}) \\]\n",
    "\n",
    "   - \\( b \\) is the bias term.\n",
    "\n",
    "These formulas describe the core operations of logistic regression, including the sigmoid activation function, the binary cross-entropy cost function, and the gradients used for updating the model parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Sigmoid function\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Binary Cross-Entropy (BCE) Cost Function\n",
    "    def BCE(self, y, a):\n",
    "        m = len(y)\n",
    "        return (-1 / m) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "    \n",
    "    # Gradient of BCE Cost Function with Respect to Weights\n",
    "    def dBCE_dw(self, x, y, a):\n",
    "        m = len(y)\n",
    "        return (1 / m) * np.sum(np.dot((a - y), x))\n",
    "\n",
    "    # Gradient of BCE Cost Function with Respect to Bias\n",
    "    def dBCE_db(self, y, a):\n",
    "        m = len(y)\n",
    "        return (1 / m) * np.sum(a - y)\n",
    "\n",
    "    # Fit the logistic regression model to the training data\n",
    "    def fit(self, x, y, learning_rate, iterations):\n",
    "        # Initialize bias and weights\n",
    "        self.b = np.array([[1.0]])\n",
    "        self.w = np.zeros((x.shape[1], 1))\n",
    "        # Set learning rate and number of iterations\n",
    "        self.lr = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # Iterate over the specified number of iterations\n",
    "        for i in range(iterations):\n",
    "            # Compute the sigmoid activation for the current weights and bias\n",
    "            a = self.sigmoid(np.dot(x, self.w) + self.b)\n",
    "            # Update the weights using the gradient of the BCE cost function\n",
    "            self.w -= self.lr * self.dBCE_dw(x, y, a)\n",
    "            # Compute the sigmoid activation again after weight update\n",
    "            a = self.sigmoid(np.dot(x, self.w) + self.b)\n",
    "            # Update the bias using the gradient of the BCE cost function\n",
    "            self.b -= self.lr * self.dBCE_db(y, a)\n",
    "            \n",
    "    # Predict the class labels for input data\n",
    "    def predict(self, x):\n",
    "        # Compute the sigmoid activation for the input data\n",
    "        return np.round(self.sigmoid(np.dot(x, self.w) + self.b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "X = np.array([[2.5], [3.5], [5.5], [6.5], [8.5]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "model.fit(X, y,.01,10000)\n",
    "X_test = np.array(list(range(0,12,1))).reshape(-1,1)\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cad00d5a13a3f60cb6d0dee52753d434f6ef86636012a882082aa2fea66afa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
